"""
Example: Chat LLM with Human-in-the-loop (HITL), Streaming, and Loop.

This example demonstrates:
1. Long-running Loop for conversation management.
2. WaitForInput for pausing/resuming execution (HITL).
3. Switch for control flow (Chat vs Exit).
4. Streaming output simulation via context bubbling.
5. Simplified Resume Logic using CompiledGraph.prepare_resume().

Run with: `python examples/chat_llm.py`
"""

import time
import sys
from typing import List, Dict, Any
from pydantic import BaseModel, Field
from taskpipe import (
    task, START, END, Switch, Loop, WaitForInput, 
    InMemoryExecutionContext, SuspendExecution, CompiledGraph
)

# ==========================
# 1. å®šä¹‰æ•°æ®å¥‘çº¦ (Contract)
# ==========================
class ChatContext(BaseModel):
    history: List[Dict[str, str]] = Field(default_factory=list)
    last_user_input: str = ""
    status: str = "CONTINUE"  # "CONTINUE" | "EXIT"

# ==========================
# 2. å®šä¹‰ä¸šåŠ¡åŸå­èƒ½åŠ› (Tasks)
# ==========================
@task
def llm_chat(user_input: str, history: List[Dict[str, str]], context: InMemoryExecutionContext) -> ChatContext:
    """ä¸šåŠ¡é€»è¾‘ï¼šè°ƒç”¨ LLM ç”Ÿæˆå›å¤ï¼Œæ”¯æŒæµå¼è¾“å‡º"""
    # æ¨¡æ‹Ÿ LLM ç”Ÿæˆ
    response_text = f"å¤è¯»æœº: {user_input}"
    
    # æ¨¡æ‹Ÿæµå¼æ¨é€ (Context ä¼šè‡ªåŠ¨å‘ä¸Šå†’æ³¡åˆ°ä¸»ç¨‹åº)
    for char in response_text:
        context.send_stream("DoChat", char)
        time.sleep(0.05) # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
    
    new_history = history + [
        {"role": "user", "content": user_input},
        {"role": "ai", "content": response_text}
    ]
    return ChatContext(history=new_history, last_user_input=user_input, status="CONTINUE")

@task
def exit_logic(history: List[Dict[str, str]]) -> ChatContext:
    """ä¸šåŠ¡é€»è¾‘ï¼šå¤„ç†é€€å‡º"""
    print("\n[ç³»ç»Ÿ] æ­£åœ¨ä¿å­˜ä¼šè¯è®°å½•...")
    return ChatContext(history=history, status="EXIT")

# ==========================
# 3. æ ¸å¿ƒï¼šæ„å»ºå•è½®å¯¹è¯å­å›¾
# ==========================
def build_chat_round_graph():
    # --- A. å®ä¾‹åŒ–èŠ‚ç‚¹ ---
    start_node = START(ChatContext, name="Start")
    
    # æš‚åœç‚¹ï¼šç­‰å¾…äººç±»è¾“å…¥
    # æ³¨æ„ï¼šWaitForInput é»˜è®¤è¾“å‡º {"result": ...}
    wait_node = WaitForInput(name="HumanInput")
    
    # é€»è¾‘åˆ¤æ–­ï¼šå¦‚æœè¾“å…¥æ˜¯ /bye åˆ™é€€å‡º
    switch_node = Switch(
        config={
            "rules": [("user_input == '/bye'", "EXIT_BRANCH")],
            "default_branch": "CHAT_BRANCH"
        },
        name="Router"
    )
    
    # åˆ†æ”¯ä»»åŠ¡ï¼šèŠå¤©
    # map_inputs æŒ‡å®šæ•°æ®æ¥æºï¼ŒTaskPipe ä¼šè‡ªåŠ¨å»ºç«‹è·¨èŠ‚ç‚¹çš„æ•°æ®ä¾èµ–
    chat_branch = llm_chat(name="DoChat").map_inputs(
        user_input=wait_node.Output.result, 
        history=start_node.Output.history
    )
    
    # åˆ†æ”¯ä»»åŠ¡ï¼šé€€å‡º
    exit_branch = exit_logic(name="DoExit").map_inputs(
        history=start_node.Output.history
    )
    
    end_node = END(ChatContext, name="End")

    # --- B. é“¾å¼ç»„è£… (Pipeline DSL) ---
    # ä½¿ç”¨æ‹¬å· (switch | dict) ç¡®ä¿åˆ›å»º SwitchBranches ç»“æ„
    pipeline_def = (
        start_node
        | wait_node
        | (
            switch_node.map_inputs(user_input=wait_node.Output.result)
            | {
                "CHAT_BRANCH": chat_branch,
                "EXIT_BRANCH": exit_branch
            }
        )
        | end_node
    )
    
    # è½¬æ¢ä¸ºç¼–è¯‘å¥½çš„å›¾ (CompiledGraph)
    # è¿™ä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰çš„éšå¼æ•°æ®è¿æ¥
    return pipeline_def.to_graph(graph_name="RoundGraph").compile()

# ==========================
# 4. æ„å»ºä¸»ç¨‹åº (Outer Loop)
# ==========================
def build_app():
    # è·å–å•è½®å¯¹è¯é€»è¾‘ï¼ˆä½œä¸ºä¸€ä¸ª CompiledGraph èŠ‚ç‚¹ï¼‰
    chat_body = build_chat_round_graph()
    
    # åªè¦ status != EXITï¼Œå°±ä¸€ç›´å¾ªç¯æ‰§è¡Œ chat_body
    main_loop = Loop(
        body=chat_body,
        config={
            "condition": "status != 'EXIT'", 
            "max_loops": 10
        },
        name="MainLoop"
    )
    return main_loop

# ==========================
# 5. æ¨¡æ‹Ÿè¿è¡Œ (Mock Runtime)
# ==========================
if __name__ == "__main__":
    app = build_app()
    ctx = InMemoryExecutionContext()
    
    # æ³¨å†Œæµå¼å›è°ƒ (å®ç°æ‰“å­—æœºæ•ˆæœ)
    def on_stream(node, chunk):
        if node == "DoChat":
            print(chunk, end="", flush=True)
    ctx.register_stream_callback(on_stream)

    # åˆå§‹åŒ–æ•°æ®
    init_data = ChatContext(history=[])
    resume_payload = None
    
    print("=== AI åŠ©ç†å·²å¯åŠ¨ (è¾“å…¥ /bye é€€å‡º) ===")

    while True:
        try:
            # æ‰§è¡Œåº”ç”¨
            # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡è¿è¡Œï¼Œresume_payload ä¸º None
            # å¦‚æœæ˜¯æ¢å¤è¿è¡Œï¼Œresume_payload åŒ…å«äº†ä¹‹å‰çš„çŠ¶æ€å’Œæ–°æ³¨å…¥çš„è¾“å…¥
            result = app.invoke(init_data, context=ctx, resume_state=resume_payload)
            
            # å¦‚æœæ­£å¸¸ç»“æŸ (Loop ç»“æŸ)
            print("\n[ç³»ç»Ÿ] æµç¨‹ç»“æŸã€‚")
            break

        except SuspendExecution as e:
            # === æ•è·æš‚åœä¿¡å· (HITL) ===
            # æ­¤æ—¶å·¥ä½œæµå·²æš‚åœå¹¶ä¿å­˜äº†å®Œæ•´å¿«ç…§ (e.snapshot)
            # åœ¨çœŸå®ç³»ç»Ÿä¸­ï¼Œä½ å¯ä»¥å°† snapshot åºåˆ—åŒ–å­˜å…¥æ•°æ®åº“ï¼Œç„¶åç»“æŸè¿›ç¨‹
            
            user_text = input("\nğŸ‘¤ ä½ : ")
            
            # === æ„é€ æ¢å¤çŠ¶æ€ (Resume State) ===
            # [ä¿®æ­£] ç›´æ¥ä½¿ç”¨ CompiledGraph.prepare_resume å¤„ç†æ•´ä¸ªå¿«ç…§
            # å®ƒä¼šè‡ªåŠ¨ç©¿é€ Loop -> Graph -> æ‰¾åˆ° "HumanInput" å¹¶æ³¨å…¥æ•°æ®
            # å¼€å‘è€…å®Œå…¨ä¸éœ€è¦çŸ¥é“ snapshot å†…éƒ¨ç»“æ„ï¼ˆå¦‚ loop_cnt, child_state ç­‰ï¼‰
            
            resume_payload = CompiledGraph.prepare_resume(
                e.snapshot, 
                inputs={"HumanInput": user_text}
            )
            
            print("ğŸ¤– ", end="")